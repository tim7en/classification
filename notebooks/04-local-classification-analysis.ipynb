{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8512f4ae",
   "metadata": {},
   "source": [
    "# Local Landcover Classification Analysis for Uzbekistan\n",
    "\n",
    "This notebook processes the comprehensive satellite tiles downloaded from Google Earth Engine and performs landcover classification using your training labels.\n",
    "\n",
    "## Prerequisites:\n",
    "1. Download the GeoTIFF files from Google Drive `earthengine_exports` folder\n",
    "2. Copy them to `data/downloaded_tiles/` directory\n",
    "3. Ensure `landcover_training.geojson` is in `data/training/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81117a1e",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80399ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.features import geometry_mask\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43478c67",
   "metadata": {},
   "source": [
    "## Step 2: Define Paths and Load Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / \"data\"\n",
    "training_dir = data_dir / \"training\"\n",
    "tiles_dir = data_dir / \"downloaded_tiles\"  # Copy tiles from Google Drive here\n",
    "results_dir = data_dir / \"results\"\n",
    "models_dir = data_dir / \"models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "tiles_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Project root: {project_root}\")\n",
    "print(f\"üìÇ Training data: {training_dir}\")\n",
    "print(f\"üìÇ Tiles directory: {tiles_dir}\")\n",
    "print(f\"üìÇ Results directory: {results_dir}\")\n",
    "\n",
    "# Define the 12 landcover classes based on your training data\n",
    "LANDCOVER_CLASSES = {\n",
    "    1: 'Residential',\n",
    "    2: 'Agriculture', \n",
    "    3: 'Buildings',\n",
    "    4: 'Forest',\n",
    "    5: 'Residential_Private',\n",
    "    6: 'Roads_Highways',\n",
    "    7: 'Land_Stock',\n",
    "    8: 'Non_Residential',\n",
    "    9: 'Protected',\n",
    "    10: 'Railways',\n",
    "    11: 'Shared_Lands',\n",
    "    12: 'Water'\n",
    "}\n",
    "\n",
    "print(\"\\nüéØ Landcover Classes:\")\n",
    "for class_id, class_name in LANDCOVER_CLASSES.items():\n",
    "    print(f\"   {class_id:2d}. {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2c953",
   "metadata": {},
   "source": [
    "## Step 3: Load Training Labels from GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training GeoJSON file\n",
    "training_geojson = training_dir / \"landcover_training.geojson\"\n",
    "\n",
    "if training_geojson.exists():\n",
    "    print(f\"üìÇ Loading training data: {training_geojson}\")\n",
    "    training_gdf = gpd.read_file(training_geojson)\n",
    "    \n",
    "    # Ensure CRS is set (assuming WGS84 if not specified)\n",
    "    if training_gdf.crs is None:\n",
    "        training_gdf.set_crs('EPSG:4326', inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ Training data loaded successfully!\")\n",
    "    print(f\"   üìä Total features: {len(training_gdf):,}\")\n",
    "    print(f\"   üóÇÔ∏è  Columns: {list(training_gdf.columns)}\")\n",
    "    print(f\"   üåç CRS: {training_gdf.crs}\")\n",
    "    \n",
    "    # Analyze class distribution\n",
    "    if 'layer_id' in training_gdf.columns:\n",
    "        layer_counts = training_gdf['layer_id'].value_counts().sort_index()\n",
    "        print(f\"\\nüìà Training Data Distribution:\")\n",
    "        for layer_id, count in layer_counts.items():\n",
    "            if layer_id in LANDCOVER_CLASSES:\n",
    "                print(f\"   {layer_id:2d}. {LANDCOVER_CLASSES[layer_id]:18}: {count:6,} polygons\")\n",
    "else:\n",
    "    print(f\"‚ùå Training data not found: {training_geojson}\")\n",
    "    print(\"Please ensure landcover_training.geojson is in the training directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae21895",
   "metadata": {},
   "source": [
    "## Step 4: Load and Explore Satellite Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97abe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available tiles\n",
    "print(\"üîç Checking for downloaded tiles...\")\n",
    "print(\"\\nüì• Please copy your downloaded GeoTIFF files from Google Drive to:\")\n",
    "print(f\"   {tiles_dir}\")\n",
    "print(\"\\nExpected files:\")\n",
    "print(\"   - uzbekistan_tile_recent_3_months_comprehensive.tif\")\n",
    "print(\"   - uzbekistan_tile_summer_2023_comprehensive.tif\")\n",
    "print(\"   - uzbekistan_tile_winter_2023_2024_comprehensive.tif\")\n",
    "\n",
    "# List available TIF files\n",
    "tif_files = list(tiles_dir.glob(\"*.tif\")) + list(tiles_dir.glob(\"*.tiff\"))\n",
    "if tif_files:\n",
    "    print(f\"\\n‚úÖ Found {len(tif_files)} tile(s):\")\n",
    "    for tif_file in tif_files:\n",
    "        print(f\"   - {tif_file.name}\")\n",
    "        \n",
    "    # Select the first tile for analysis\n",
    "    selected_tile = tif_files[0]\n",
    "    print(f\"\\nüìä Selected for analysis: {selected_tile.name}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No TIF files found in the tiles directory.\")\n",
    "    print(\"   Please download and copy the tiles from Google Drive first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_tile(tile_path):\n",
    "    \"\"\"Load a satellite tile and display its properties\"\"\"\n",
    "    with rasterio.open(tile_path) as src:\n",
    "        print(f\"\\nüìä Tile Information: {tile_path.name}\")\n",
    "        print(f\"   - Shape: {src.height} x {src.width} pixels\")\n",
    "        print(f\"   - Bands: {src.count}\")\n",
    "        print(f\"   - CRS: {src.crs}\")\n",
    "        print(f\"   - Pixel Size: {src.res[0]}m x {src.res[1]}m\")\n",
    "        print(f\"   - Bounds: {src.bounds}\")\n",
    "        \n",
    "        # Expected band names based on our comprehensive export\n",
    "        expected_bands = [\n",
    "            'SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7',\n",
    "            'NDVI', 'NDWI', 'MNDWI', 'NDBI', 'EVI', 'SAVI',\n",
    "            'elevation', 'SLOPE', 'ASPECT', 'HILLSHADE',\n",
    "            'TERRAIN_MOUNTAIN', 'WATER_MASK',\n",
    "            'VEG_SPARSE', 'VEG_MODERATE', 'VEG_DENSE',\n",
    "            'TEMP_C', 'NIR_TEXTURE'\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n   üìä Expected Bands (based on GEE export):\")\n",
    "        for i, name in enumerate(expected_bands[:src.count], 1):\n",
    "            print(f\"      Band {i:2d}: {name}\")\n",
    "        \n",
    "        return src.meta, expected_bands[:src.count]\n",
    "\n",
    "# Load and explore the selected tile\n",
    "if tif_files:\n",
    "    tile_meta, band_names = load_and_explore_tile(selected_tile)\n",
    "    print(f\"\\n‚úÖ Ready for analysis with {len(band_names)} bands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660fc10",
   "metadata": {},
   "source": [
    "## Step 5: Extract Training Samples from Satellite Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import gc  # For garbage collection to manage memory\n",
    "\n",
    "def clip_raster_by_polygon(raster_path, polygon_gdf, output_path=None):\n",
    "    \"\"\"\n",
    "    Clip raster by polygon boundaries to reduce processing load.\n",
    "    \"\"\"\n",
    "    import rasterio.mask\n",
    "    \n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Get the polygon geometries\n",
    "        geometries = polygon_gdf.geometry.values\n",
    "        \n",
    "        # Clip the raster\n",
    "        clipped_data, clipped_transform = rasterio.mask.mask(\n",
    "            src, geometries, crop=True, nodata=src.nodata\n",
    "        )\n",
    "        \n",
    "        # Update metadata\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\n",
    "            'height': clipped_data.shape[1],\n",
    "            'width': clipped_data.shape[2],\n",
    "            'transform': clipped_transform\n",
    "        })\n",
    "        \n",
    "        if output_path:\n",
    "            with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "                dst.write(clipped_data)\n",
    "            return output_path, clipped_meta\n",
    "        else:\n",
    "            return clipped_data, clipped_meta, clipped_transform\n",
    "\n",
    "def extract_pixel_values_efficient(raster_path, polygons_gdf, class_column='layer_id', \n",
    "                                 region_name=\"region\", max_samples_per_class=5000):\n",
    "    \"\"\"\n",
    "    Extract pixel values from raster for each polygon with memory optimization.\n",
    "    Returns features (X) and labels (y) for training.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Extracting features from {region_name}...\")\n",
    "    \n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Get raster properties\n",
    "        n_bands = src.count\n",
    "        print(f\"   \udcca Processing {n_bands} bands\")\n",
    "        \n",
    "        # Process each class separately to control memory usage\n",
    "        unique_classes = polygons_gdf[class_column].unique()\n",
    "        print(f\"   üè∑Ô∏è  Found {len(unique_classes)} classes: {unique_classes}\")\n",
    "        \n",
    "        for class_id in unique_classes:\n",
    "            print(f\"   Processing class {class_id}...\")\n",
    "            \n",
    "            # Get polygons for this class\n",
    "            class_polygons = polygons_gdf[polygons_gdf[class_column] == class_id]\n",
    "            \n",
    "            # Extract pixels for this class\n",
    "            class_features = []\n",
    "            \n",
    "            for idx, polygon in class_polygons.iterrows():\n",
    "                try:\n",
    "                    # Create a mask for this polygon\n",
    "                    geometry = [polygon.geometry]\n",
    "                    masked_data, masked_transform = rasterio.mask.mask(\n",
    "                        src, geometry, crop=True, nodata=src.nodata, filled=False\n",
    "                    )\n",
    "                    \n",
    "                    # Get valid pixels (not nodata)\n",
    "                    valid_mask = ~masked_data.mask[0]  # Use first band for mask\n",
    "                    \n",
    "                    if valid_mask.sum() > 0:  # If there are valid pixels\n",
    "                        # Extract all bands for valid pixels\n",
    "                        pixel_values = masked_data[:, valid_mask].T  # Shape: (n_pixels, n_bands)\n",
    "                        \n",
    "                        # Limit samples per polygon to manage memory\n",
    "                        if len(pixel_values) > 1000:\n",
    "                            indices = np.random.choice(len(pixel_values), 1000, replace=False)\n",
    "                            pixel_values = pixel_values[indices]\n",
    "                        \n",
    "                        class_features.append(pixel_values)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"     ‚ö†Ô∏è  Error processing polygon {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if class_features:\n",
    "                # Combine all features for this class\n",
    "                class_features_combined = np.vstack(class_features)\n",
    "                \n",
    "                # Limit total samples per class\n",
    "                if len(class_features_combined) > max_samples_per_class:\n",
    "                    indices = np.random.choice(len(class_features_combined), \n",
    "                                             max_samples_per_class, replace=False)\n",
    "                    class_features_combined = class_features_combined[indices]\n",
    "                \n",
    "                features_list.append(class_features_combined)\n",
    "                labels_list.extend([class_id] * len(class_features_combined))\n",
    "                \n",
    "                print(f\"     ‚úÖ Extracted {len(class_features_combined)} samples for class {class_id}\")\n",
    "            \n",
    "            # Clean up memory\n",
    "            gc.collect()\n",
    "    \n",
    "    if features_list:\n",
    "        X = np.vstack(features_list)\n",
    "        y = np.array(labels_list)\n",
    "        \n",
    "        print(f\"\\nüìä Final dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "        print(f\"   Class distribution: {np.unique(y, return_counts=True)}\")\n",
    "        \n",
    "        return X, y\n",
    "    else:\n",
    "        print(\"‚ùå No valid samples extracted!\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44905778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTemporalRegionalClassifier:\n",
    "    \"\"\"\n",
    "    A classifier that handles multiple time periods and regional processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir=\"models\"):\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(exist_ok=True)\n",
    "        self.models = {}\n",
    "        self.band_names = None\n",
    "        \n",
    "    def train_regional_model(self, raster_paths, training_gdf, region_polygons=None,\n",
    "                           model_name=\"regional_model\", test_size=0.2):\n",
    "        \"\"\"\n",
    "        Train a model using multiple raster files and regional clipping.\n",
    "        \n",
    "        Args:\n",
    "            raster_paths: Dict with keys like {'recent': path, 'summer': path, 'winter': path}\n",
    "            training_gdf: GeoDataFrame with training polygons\n",
    "            region_polygons: Optional GeoDataFrame for regional clipping\n",
    "            model_name: Name for saving the model\n",
    "        \"\"\"\n",
    "        print(f\"\\nüéØ Training {model_name} model...\")\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Process each time period\n",
    "        for period, raster_path in raster_paths.items():\n",
    "            print(f\"\\nüìÖ Processing {period} data...\")\n",
    "            \n",
    "            # Clip to region if specified\n",
    "            if region_polygons is not None:\n",
    "                print(\"   üîß Clipping raster to region...\")\n",
    "                clipped_data, clipped_meta, clipped_transform = clip_raster_by_polygon(\n",
    "                    raster_path, region_polygons\n",
    "                )\n",
    "                \n",
    "                # Create temporary clipped file\n",
    "                temp_path = self.model_dir / f\"temp_{period}_clipped.tif\"\n",
    "                with rasterio.open(temp_path, 'w', **clipped_meta) as dst:\n",
    "                    dst.write(clipped_data)\n",
    "                \n",
    "                # Extract features from clipped raster\n",
    "                X_period, y_period = extract_pixel_values_efficient(\n",
    "                    temp_path, training_gdf, region_name=f\"{period}_region\"\n",
    "                )\n",
    "                \n",
    "                # Clean up temporary file\n",
    "                temp_path.unlink()\n",
    "            else:\n",
    "                # Extract features from full raster\n",
    "                X_period, y_period = extract_pixel_values_efficient(\n",
    "                    raster_path, training_gdf, region_name=period\n",
    "                )\n",
    "            \n",
    "            if X_period is not None:\n",
    "                all_features.append(X_period)\n",
    "                all_labels.append(y_period)\n",
    "        \n",
    "        if not all_features:\n",
    "            print(\"‚ùå No features extracted from any time period!\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all time periods\n",
    "        print(\"\\nüîÑ Combining multi-temporal features...\")\n",
    "        X_combined = np.vstack(all_features)\n",
    "        y_combined = np.hstack(all_labels)\n",
    "        \n",
    "        print(f\"üìä Combined dataset: {X_combined.shape[0]} samples, {X_combined.shape[1]} features\")\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_combined, test_size=test_size, random_state=42, stratify=y_combined\n",
    "        )\n",
    "        \n",
    "        # Train Random Forest model\n",
    "        print(\"\\nüå≤ Training Random Forest classifier...\")\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        print(\"\\nüìà Model evaluation...\")\n",
    "        train_score = rf_model.score(X_train, y_train)\n",
    "        test_score = rf_model.score(X_test, y_test)\n",
    "        \n",
    "        print(f\"   Training accuracy: {train_score:.3f}\")\n",
    "        print(f\"   Testing accuracy: {test_score:.3f}\")\n",
    "        \n",
    "        # Detailed classification report\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        print(\"\\nüìã Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Save model\n",
    "        model_path = self.model_dir / f\"{model_name}.pkl\"\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'model': rf_model,\n",
    "                'band_names': self.band_names,\n",
    "                'class_names': training_gdf['layer_id'].unique(),\n",
    "                'training_info': {\n",
    "                    'train_samples': len(X_train),\n",
    "                    'test_samples': len(X_test),\n",
    "                    'train_accuracy': train_score,\n",
    "                    'test_accuracy': test_score\n",
    "                }\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"üíæ Model saved to: {model_path}\")\n",
    "        \n",
    "        # Store in memory\n",
    "        self.models[model_name] = rf_model\n",
    "        \n",
    "        return rf_model, test_score\n",
    "    \n",
    "    def load_model(self, model_name):\n",
    "        \"\"\"Load a saved model\"\"\"\n",
    "        model_path = self.model_dir / f\"{model_name}.pkl\"\n",
    "        \n",
    "        if model_path.exists():\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            \n",
    "            self.models[model_name] = model_data['model']\n",
    "            self.band_names = model_data['band_names']\n",
    "            \n",
    "            print(f\"‚úÖ Model {model_name} loaded successfully\")\n",
    "            print(f\"   Training accuracy: {model_data['training_info']['train_accuracy']:.3f}\")\n",
    "            print(f\"   Testing accuracy: {model_data['training_info']['test_accuracy']:.3f}\")\n",
    "            \n",
    "            return model_data['model']\n",
    "        else:\n",
    "            print(f\"‚ùå Model file not found: {model_path}\")\n",
    "            return None\n",
    "    \n",
    "    def classify_large_raster(self, raster_path, model_name, output_path, \n",
    "                            chunk_size=1024, region_polygons=None):\n",
    "        \"\"\"\n",
    "        Classify a large raster using chunked processing to manage memory.\n",
    "        \"\"\"\n",
    "        print(f\"\\nüó∫Ô∏è  Classifying large raster: {raster_path}\")\n",
    "        \n",
    "        if model_name not in self.models:\n",
    "            self.load_model(model_name)\n",
    "        \n",
    "        if model_name not in self.models:\n",
    "            print(f\"‚ùå Model {model_name} not available!\")\n",
    "            return None\n",
    "        \n",
    "        model = self.models[model_name]\n",
    "        \n",
    "        with rasterio.open(raster_path) as src:\n",
    "            # Get raster properties\n",
    "            profile = src.profile.copy()\n",
    "            profile.update(dtype=rasterio.uint8, count=1, compress='lzw')\n",
    "            \n",
    "            height, width = src.height, src.width\n",
    "            \n",
    "            print(f\"   üìä Raster size: {height} x {width} pixels\")\n",
    "            print(f\"   üîß Processing in {chunk_size}x{chunk_size} chunks...\")\n",
    "            \n",
    "            # Create output raster\n",
    "            with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                # Process in chunks\n",
    "                for row in range(0, height, chunk_size):\n",
    "                    for col in range(0, width, chunk_size):\n",
    "                        # Define window\n",
    "                        window = rasterio.windows.Window(\n",
    "                            col, row, \n",
    "                            min(chunk_size, width - col),\n",
    "                            min(chunk_size, height - row)\n",
    "                        )\n",
    "                        \n",
    "                        # Read chunk\n",
    "                        chunk_data = src.read(window=window)  # Shape: (bands, height, width)\n",
    "                        \n",
    "                        # Reshape for prediction\n",
    "                        chunk_height, chunk_width = chunk_data.shape[1], chunk_data.shape[2]\n",
    "                        chunk_reshaped = chunk_data.reshape(chunk_data.shape[0], -1).T\n",
    "                        \n",
    "                        # Handle nodata values\n",
    "                        valid_mask = ~np.isnan(chunk_reshaped).any(axis=1)\n",
    "                        \n",
    "                        if valid_mask.sum() > 0:\n",
    "                            # Predict only valid pixels\n",
    "                            valid_pixels = chunk_reshaped[valid_mask]\n",
    "                            predictions = model.predict(valid_pixels)\n",
    "                            \n",
    "                            # Create output chunk\n",
    "                            output_chunk = np.full(chunk_height * chunk_width, 0, dtype=np.uint8)\n",
    "                            output_chunk[valid_mask] = predictions\n",
    "                            output_chunk = output_chunk.reshape(chunk_height, chunk_width)\n",
    "                        else:\n",
    "                            output_chunk = np.zeros((chunk_height, chunk_width), dtype=np.uint8)\n",
    "                        \n",
    "                        # Write chunk\n",
    "                        dst.write(output_chunk[np.newaxis, :, :], window=window)\n",
    "                        \n",
    "                        print(f\"     ‚úÖ Processed chunk ({row}:{row+chunk_height}, {col}:{col+chunk_width})\")\n",
    "        \n",
    "        print(f\"üéâ Classification complete! Saved to: {output_path}\")\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07c1ab",
   "metadata": {},
   "source": [
    "## Step 6: Regional Processing Implementation\n",
    "\n",
    "Now let's implement the regional processing strategy for your large territory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a91b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the regional classifier\n",
    "classifier = MultiTemporalRegionalClassifier(model_dir=\"models\")\n",
    "\n",
    "# Load regional boundaries (Tashkent borders for regional processing)\n",
    "print(\"üó∫Ô∏è  Loading regional boundaries...\")\n",
    "tashkent_borders = gpd.read_file(\"../data/training/Tashkent_borders.geojson\")\n",
    "print(f\"   ‚úÖ Loaded {len(tashkent_borders)} boundary polygons\")\n",
    "\n",
    "# Define your three raster files (update paths as needed)\n",
    "raster_files = {\n",
    "    'recent_3_months': \"../data/downloaded_tiles/uzbekistan_tile_recent_3_months_comprehensive.tif\",\n",
    "    'summer_2023': \"../data/downloaded_tiles/uzbekistan_tile_summer_2023_comprehensive.tif\", \n",
    "    'winter_2023_2024': \"../data/downloaded_tiles/uzbekistan_tile_winter_2023_2024_comprehensive.tif\"\n",
    "}\n",
    "\n",
    "# Check which files exist\n",
    "available_rasters = {}\n",
    "for period, path in raster_files.items():\n",
    "    if Path(path).exists():\n",
    "        available_rasters[period] = path\n",
    "        print(f\"‚úÖ Found {period}: {Path(path).name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Missing {period}: {path}\")\n",
    "\n",
    "print(f\"\\nüìä Available for processing: {len(available_rasters)} raster files\")\n",
    "\n",
    "# Store band names for reference\n",
    "if available_rasters:\n",
    "    # Use the first available raster to get band information\n",
    "    first_raster = list(available_rasters.values())[0]\n",
    "    classifier.band_names = [\n",
    "        'SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7',\n",
    "        'NDVI', 'NDWI', 'MNDWI', 'NDBI', 'EVI', 'SAVI',\n",
    "        'elevation', 'SLOPE', 'ASPECT', 'HILLSHADE',\n",
    "        'TERRAIN_MOUNTAIN', 'WATER_MASK',\n",
    "        'VEG_SPARSE', 'VEG_MODERATE', 'VEG_DENSE',\n",
    "        'TEMP_C', 'NIR_TEXTURE'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGY 1: Train on selected regions first\n",
    "print(\"\\nüéØ STRATEGY 1: Regional Model Training\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if available_rasters and training_data is not None:\n",
    "    # Option A: Train on a subset of Tashkent (faster, good for testing)\n",
    "    print(\"\\nüìç Option A: Training on Tashkent region subset...\")\n",
    "    \n",
    "    # You can modify this to select specific regions or use a bounding box\n",
    "    # For now, let's use the first polygon as a training region\n",
    "    training_region = tashkent_borders.iloc[:1].copy()  # First polygon only\n",
    "    \n",
    "    try:\n",
    "        # Train the regional model\n",
    "        model, accuracy = classifier.train_regional_model(\n",
    "            raster_paths=available_rasters,\n",
    "            training_gdf=training_data,\n",
    "            region_polygons=training_region,\n",
    "            model_name=\"tashkent_regional_model\"\n",
    "        )\n",
    "        \n",
    "        if model is not None:\n",
    "            print(f\"\\n‚úÖ Regional model trained successfully!\")\n",
    "            print(f\"   üéØ Accuracy: {accuracy:.3f}\")\n",
    "            print(f\"   üíæ Model saved for reuse\")\n",
    "            \n",
    "            # Show feature importance\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': classifier.band_names,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(\"\\nüèÜ Top 10 Most Important Features:\")\n",
    "                print(feature_importance.head(10).to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in regional training: {e}\")\n",
    "        print(\"   üí° This might be due to memory constraints or data issues\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping training - missing raster files or training data\")\n",
    "    print(\"   Please ensure you have downloaded the GeoTIFF files from Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGY 2: Apply trained model to entire territory\n",
    "print(\"\\nüó∫Ô∏è  STRATEGY 2: Large-Scale Classification\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"../data/results/classifications\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if we have a trained model\n",
    "model_path = Path(\"models/tashkent_regional_model.pkl\")\n",
    "if model_path.exists() or \"tashkent_regional_model\" in classifier.models:\n",
    "    print(\"‚úÖ Trained model available for classification\")\n",
    "    \n",
    "    # Classify each time period\n",
    "    for period, raster_path in available_rasters.items():\n",
    "        print(f\"\\nüìÖ Classifying {period} data...\")\n",
    "        \n",
    "        output_path = output_dir / f\"landcover_classification_{period}.tif\"\n",
    "        \n",
    "        try:\n",
    "            # Use chunked processing for large rasters\n",
    "            result_path = classifier.classify_large_raster(\n",
    "                raster_path=raster_path,\n",
    "                model_name=\"tashkent_regional_model\",\n",
    "                output_path=str(output_path),\n",
    "                chunk_size=512,  # Adjust based on your memory capacity\n",
    "                region_polygons=None  # Set to tashkent_borders if you want to clip to region\n",
    "            )\n",
    "            \n",
    "            if result_path:\n",
    "                print(f\"‚úÖ Classification saved: {result_path}\")\n",
    "                \n",
    "                # Quick stats about the classification\n",
    "                with rasterio.open(result_path) as src:\n",
    "                    classified_data = src.read(1)\n",
    "                    unique_classes, counts = np.unique(classified_data[classified_data > 0], return_counts=True)\n",
    "                    \n",
    "                    print(f\"   üìä Classification Statistics for {period}:\")\n",
    "                    for class_id, count in zip(unique_classes, counts):\n",
    "                        percentage = (count / counts.sum()) * 100\n",
    "                        print(f\"      Class {class_id}: {count:,} pixels ({percentage:.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error classifying {period}: {e}\")\n",
    "            print(\"   üí° Try reducing chunk_size or using regional clipping\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No trained model found!\")\n",
    "    print(\"   Please run the training step first or load an existing model\")\n",
    "    \n",
    "    # Show how to load an existing model\n",
    "    print(\"\\nüí° To load an existing model:\")\n",
    "    print(\"   classifier.load_model('tashkent_regional_model')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATEGY 3: Multi-temporal analysis and visualization\n",
    "print(\"\\nüìä STRATEGY 3: Multi-temporal Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def visualize_classification_results():\n",
    "    \"\"\"Visualize and compare classification results across time periods\"\"\"\n",
    "    \n",
    "    # Look for classification results\n",
    "    classification_files = list(output_dir.glob(\"landcover_classification_*.tif\"))\n",
    "    \n",
    "    if not classification_files:\n",
    "        print(\"‚ùå No classification results found!\")\n",
    "        print(\"   Run the classification step first\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìà Found {len(classification_files)} classification results\")\n",
    "    \n",
    "    # Create a figure for comparison\n",
    "    n_files = len(classification_files)\n",
    "    fig, axes = plt.subplots(1, n_files, figsize=(6*n_files, 6))\n",
    "    if n_files == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Define class colors (you can customize these)\n",
    "    class_colors = {\n",
    "        1: '#8B4513',  # Residential - Brown\n",
    "        2: '#FFD700',  # Agriculture - Gold\n",
    "        3: '#696969',  # Buildings - Dark Gray\n",
    "        4: '#228B22',  # Forest - Forest Green\n",
    "        5: '#DDA0DD',  # Residential_Private - Plum\n",
    "        6: '#2F4F4F',  # Roads_Highways - Dark Slate Gray\n",
    "        7: '#D2B48C',  # Land_Stock - Tan\n",
    "        8: '#800080',  # Non_Residential - Purple\n",
    "        9: '#32CD32',  # Protected - Lime Green\n",
    "        10: '#A0A0A0', # Railways - Gray\n",
    "        11: '#90EE90', # Shared_Lands - Light Green\n",
    "        12: '#4169E1'  # Water - Royal Blue\n",
    "    }\n",
    "    \n",
    "    for i, file_path in enumerate(classification_files):\n",
    "        period = file_path.stem.replace('landcover_classification_', '')\n",
    "        \n",
    "        with rasterio.open(file_path) as src:\n",
    "            classified_data = src.read(1)\n",
    "            \n",
    "            # Create custom colormap\n",
    "            from matplotlib.colors import ListedColormap\n",
    "            colors = [class_colors.get(i, '#000000') for i in range(13)]\n",
    "            cmap = ListedColormap(colors)\n",
    "            \n",
    "            # Plot\n",
    "            im = axes[i].imshow(classified_data, cmap=cmap, vmin=0, vmax=12)\n",
    "            axes[i].set_title(f'{period.replace(\"_\", \" \").title()}', fontsize=12)\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "            # Add statistics\n",
    "            unique_classes, counts = np.unique(classified_data[classified_data > 0], return_counts=True)\n",
    "            total_pixels = counts.sum()\n",
    "            \n",
    "            stats_text = f\"Classes: {len(unique_classes)}\\nPixels: {total_pixels:,}\"\n",
    "            axes[i].text(0.02, 0.98, stats_text, transform=axes[i].transAxes, \n",
    "                        verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                        facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"multi_temporal_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return classification_files\n",
    "\n",
    "def analyze_temporal_changes(classification_files):\n",
    "    \"\"\"Analyze changes between different time periods\"\"\"\n",
    "    \n",
    "    if len(classification_files) < 2:\n",
    "        print(\"‚ö†Ô∏è  Need at least 2 time periods for change analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüîÑ Temporal Change Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Load all classifications\n",
    "    classifications = {}\n",
    "    for file_path in classification_files:\n",
    "        period = file_path.stem.replace('landcover_classification_', '')\n",
    "        with rasterio.open(file_path) as src:\n",
    "            classifications[period] = src.read(1)\n",
    "    \n",
    "    # Compare each pair of time periods\n",
    "    periods = list(classifications.keys())\n",
    "    for i in range(len(periods)-1):\n",
    "        period1, period2 = periods[i], periods[i+1]\n",
    "        \n",
    "        print(f\"\\nüìÖ Comparing {period1} ‚Üí {period2}\")\n",
    "        \n",
    "        data1 = classifications[period1]\n",
    "        data2 = classifications[period2]\n",
    "        \n",
    "        # Calculate change matrix\n",
    "        valid_mask = (data1 > 0) & (data2 > 0)\n",
    "        changes = (data1[valid_mask] != data2[valid_mask]).sum()\n",
    "        total_valid = valid_mask.sum()\n",
    "        \n",
    "        change_percentage = (changes / total_valid) * 100 if total_valid > 0 else 0\n",
    "        \n",
    "        print(f\"   üîÑ Changed pixels: {changes:,} ({change_percentage:.1f}%)\")\n",
    "        print(f\"   ‚úÖ Stable pixels: {total_valid - changes:,} ({100-change_percentage:.1f}%)\")\n",
    "        \n",
    "        # Find most common changes\n",
    "        change_pairs = []\n",
    "        for old_class in np.unique(data1[valid_mask]):\n",
    "            for new_class in np.unique(data2[valid_mask]):\n",
    "                if old_class != new_class:\n",
    "                    change_count = ((data1 == old_class) & (data2 == new_class)).sum()\n",
    "                    if change_count > 0:\n",
    "                        change_pairs.append((old_class, new_class, change_count))\n",
    "        \n",
    "        # Sort by frequency and show top changes\n",
    "        change_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "        print(f\"   üèÜ Top 5 land cover changes:\")\n",
    "        for old_c, new_c, count in change_pairs[:5]:\n",
    "            print(f\"      Class {old_c} ‚Üí Class {new_c}: {count:,} pixels\")\n",
    "\n",
    "# Run the visualization and analysis\n",
    "classification_results = visualize_classification_results()\n",
    "if classification_results:\n",
    "    analyze_temporal_changes(classification_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae73604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training samples if we have both tiles and training data\n",
    "if tif_files and 'training_gdf' in locals():\n",
    "    print(\"üöÄ Starting feature extraction...\")\n",
    "    X, y = extract_pixel_values(selected_tile, training_gdf)\n",
    "    \n",
    "    if X is not None:\n",
    "        # Remove any pixels with NaN or infinite values\n",
    "        valid_pixels = ~np.any(np.isnan(X) | np.isinf(X), axis=1)\n",
    "        X = X[valid_pixels]\n",
    "        y = y[valid_pixels]\n",
    "        \n",
    "        print(f\"\\nüìä Final dataset after cleaning:\")\n",
    "        print(f\"   - Features shape: {X.shape}\")\n",
    "        print(f\"   - Labels shape: {y.shape}\")\n",
    "        print(f\"   - Unique classes: {sorted(np.unique(y))}\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        print(\"\\nüìà Class distribution in extracted pixels:\")\n",
    "        for class_id in sorted(np.unique(y)):\n",
    "            count = np.sum(y == class_id)\n",
    "            percentage = (count / len(y)) * 100\n",
    "            class_name = LANDCOVER_CLASSES.get(class_id, f\"Unknown_{class_id}\")\n",
    "            print(f\"   {class_id:2d}. {class_name:18}: {count:8,} pixels ({percentage:5.2f}%)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Missing tiles or training data. Please ensure both are available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5708c",
   "metadata": {},
   "source": [
    "## Step 6: Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X' in locals() and X is not None:\n",
    "    # Split data into training and testing sets\n",
    "    print(\"üîÑ Splitting data into train/test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"   Training set: {X_train.shape[0]:,} pixels\")\n",
    "    print(f\"   Test set: {X_test.shape[0]:,} pixels\")\n",
    "    \n",
    "    # Train Random Forest classifier\n",
    "    print(\"\\nüå≤ Training Random Forest Classifier...\")\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    print(\"‚úÖ Model training complete!\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nüìä Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = models_dir / \"rf_landcover_classifier.joblib\"\n",
    "    joblib.dump(rf_classifier, model_path)\n",
    "    print(f\"\\nüíæ Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31b57e",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rf_classifier' in locals():\n",
    "    # Generate classification report\n",
    "    print(\"\\nüìä Classification Report:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create label names for the report\n",
    "    unique_classes = sorted(np.unique(y_test))\n",
    "    class_names = [LANDCOVER_CLASSES[i] for i in unique_classes]\n",
    "    \n",
    "    report = classification_report(\n",
    "        y_test, y_pred,\n",
    "        labels=unique_classes,\n",
    "        target_names=class_names,\n",
    "        digits=3\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = rf_classifier.feature_importances_\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': band_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüéØ Top 10 Most Important Features:\")\n",
    "    print(\"=\"*40)\n",
    "    for idx, row in importance_df.head(10).iterrows():\n",
    "        print(f\"{row['feature']:20} : {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "if 'rf_classifier' in locals():\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=unique_classes)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.title('Confusion Matrix - Landcover Classification', fontsize=16)\n",
    "    plt.xlabel('Predicted Class', fontsize=12)\n",
    "    plt.ylabel('True Class', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(results_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüíæ Confusion matrix saved to: {results_dir / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7c0e6",
   "metadata": {},
   "source": [
    "## Step 8: Apply Model to Create Landcover Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_raster(raster_path, model, output_path, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Apply the trained model to classify the entire raster.\n",
    "    Process in chunks to manage memory.\n",
    "    \"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Get metadata for output\n",
    "        meta = src.meta.copy()\n",
    "        meta.update({\n",
    "            'dtype': 'uint8',\n",
    "            'count': 1,\n",
    "            'compress': 'lzw'\n",
    "        })\n",
    "        \n",
    "        print(f\"üìä Processing raster: {src.height} x {src.width} pixels\")\n",
    "        \n",
    "        # Create output raster\n",
    "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "            # Process in chunks\n",
    "            total_chunks = (src.height + chunk_size - 1) // chunk_size\n",
    "            \n",
    "            for i, row_start in enumerate(range(0, src.height, chunk_size)):\n",
    "                # Calculate chunk boundaries\n",
    "                row_end = min(row_start + chunk_size, src.height)\n",
    "                \n",
    "                print(f\"   Processing chunk {i+1}/{total_chunks}: rows {row_start}-{row_end}\")\n",
    "                \n",
    "                # Read chunk\n",
    "                window = rasterio.windows.Window(\n",
    "                    0, row_start, \n",
    "                    src.width, row_end - row_start\n",
    "                )\n",
    "                chunk_data = src.read(window=window)\n",
    "                \n",
    "                # Reshape for prediction (pixels x bands)\n",
    "                n_bands, n_rows, n_cols = chunk_data.shape\n",
    "                chunk_reshaped = chunk_data.transpose(1, 2, 0).reshape(-1, n_bands)\n",
    "                \n",
    "                # Handle NaN and infinite values\n",
    "                valid_pixels = ~np.any(np.isnan(chunk_reshaped) | np.isinf(chunk_reshaped), axis=1)\n",
    "                predictions = np.zeros(chunk_reshaped.shape[0], dtype=np.uint8)\n",
    "                \n",
    "                if np.any(valid_pixels):\n",
    "                    # Predict only on valid pixels\n",
    "                    predictions[valid_pixels] = model.predict(chunk_reshaped[valid_pixels])\n",
    "                \n",
    "                # Reshape back and write\n",
    "                predictions_2d = predictions.reshape(n_rows, n_cols)\n",
    "                dst.write(predictions_2d, 1, window=window)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Classification complete! Output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ef634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to create a landcover map\n",
    "if 'rf_classifier' in locals() and tif_files:\n",
    "    print(\"üó∫Ô∏è  Creating landcover classification map...\")\n",
    "    \n",
    "    # Define output path\n",
    "    output_map = results_dir / f\"landcover_map_{selected_tile.stem}.tif\"\n",
    "    \n",
    "    # Classify the raster\n",
    "    classify_raster(selected_tile, rf_classifier, output_map)\n",
    "    \n",
    "    print(f\"\\nüéâ Landcover map created successfully!\")\n",
    "    print(f\"   üìç Location: {output_map}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Model or tiles not available for classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682fe484",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample of the classification result\n",
    "if 'output_map' in locals() and output_map.exists():\n",
    "    with rasterio.open(output_map) as src:\n",
    "        # Read a sample area (center of the image)\n",
    "        h, w = src.height, src.width\n",
    "        sample_size = min(2000, h, w)\n",
    "        \n",
    "        window = rasterio.windows.Window(\n",
    "            (w - sample_size) // 2,\n",
    "            (h - sample_size) // 2,\n",
    "            sample_size,\n",
    "            sample_size\n",
    "        )\n",
    "        \n",
    "        sample_data = src.read(1, window=window)\n",
    "        \n",
    "        # Create color map\n",
    "        from matplotlib.colors import ListedColormap\n",
    "        colors = [\n",
    "            '#8B0000',  # 1. Residential (Dark Red)\n",
    "            '#32CD32',  # 2. Agriculture (Lime Green)\n",
    "            '#A0522D',  # 3. Buildings (Sienna)\n",
    "            '#228B22',  # 4. Forest (Forest Green)\n",
    "            '#FF6347',  # 5. Residential_Private (Tomato)\n",
    "            '#696969',  # 6. Roads_Highways (Dim Gray)\n",
    "            '#DEB887',  # 7. Land_Stock (Burlywood)\n",
    "            '#800080',  # 8. Non_Residential (Purple)\n",
    "            '#90EE90',  # 9. Protected (Light Green)\n",
    "            '#2F4F4F',  # 10. Railways (Dark Slate Gray)\n",
    "            '#F4A460',  # 11. Shared_Lands (Sandy Brown)\n",
    "            '#4169E1'   # 12. Water (Royal Blue)\n",
    "        ]\n",
    "        \n",
    "        cmap = ListedColormap(colors[:len(LANDCOVER_CLASSES)])\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(15, 12))\n",
    "        im = ax.imshow(sample_data, cmap=cmap, vmin=1, vmax=len(LANDCOVER_CLASSES))\n",
    "        ax.set_title('Landcover Classification Sample (Center Region)', fontsize=16)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add colorbar with class labels\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04, shrink=0.8)\n",
    "        cbar.set_ticks(list(LANDCOVER_CLASSES.keys()))\n",
    "        cbar.set_ticklabels([f\"{k}. {v}\" for k, v in LANDCOVER_CLASSES.items()])\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_dir / 'landcover_sample_visualization.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüíæ Visualization saved to: {results_dir / 'landcover_sample_visualization.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0751b55b",
   "metadata": {},
   "source": [
    "## Step 10: Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ LANDCOVER CLASSIFICATION ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'rf_classifier' in locals():\n",
    "    print(f\"\\nüìä Model Performance:\")\n",
    "    print(f\"   ‚Ä¢ Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Number of training pixels: {X_train.shape[0]:,}\")\n",
    "    print(f\"   ‚Ä¢ Number of features: {X_train.shape[1]}\")\n",
    "    print(f\"   ‚Ä¢ Number of classes: {len(np.unique(y))}\")\n",
    "\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "output_files = [\n",
    "    (models_dir / \"rf_landcover_classifier.joblib\", \"Trained Model\"),\n",
    "    (results_dir / f\"landcover_map_{selected_tile.stem}.tif\", \"Landcover Map\"),\n",
    "    (results_dir / 'confusion_matrix.png', \"Confusion Matrix\"),\n",
    "    (results_dir / 'landcover_sample_visualization.png', \"Sample Visualization\")\n",
    "]\n",
    "\n",
    "for file_path, description in output_files:\n",
    "    if file_path.exists():\n",
    "        print(f\"   ‚úÖ {description}: {file_path}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {description}: Not created\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Review the confusion matrix to identify classes that need improvement\")\n",
    "print(\"   2. Analyze feature importance to understand which bands are most useful\")\n",
    "print(\"   3. Consider adding more training data for underrepresented classes\")\n",
    "print(\"   4. Apply the model to other seasonal tiles for temporal analysis\")\n",
    "print(\"   5. Export results to GIS software (QGIS, ArcGIS) for further analysis\")\n",
    "print(\"   6. Consider hyperparameter tuning or ensemble methods for improved accuracy\")\n",
    "print(\"   7. Validate results with ground truth data if available\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
